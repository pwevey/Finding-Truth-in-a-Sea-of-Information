"""
generate_sitemap.py — Auto-generate sitemap.xml and feed.xml from HTML files in /docs.

Usage:
    python generate_sitemap.py

Scans all .html files in /docs (except 404.html), extracts metadata from
<meta> tags, and regenerates sitemap.xml and feed.xml.

Run this after adding or updating any article.
"""

import os
import re
import html
from datetime import datetime, timezone

DOCS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'docs')
DOMAIN = 'https://truthandai.org'
SITE_TITLE = 'Truth &amp; Artificial Intelligence'
SITE_DESCRIPTION = ('Christian theological and technical reflections on Artificial '
                    'Intelligence and Large Language Models. Truth is grounded in '
                    'God — not generated by machines.')
AUTHOR = 'Paul Wever'
EXCLUDE = {'404.html'}


def extract_meta(filepath):
    """Extract metadata from an HTML file's <head> section."""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    def get_meta(name, attr='name'):
        pattern = rf'<meta\s+(?:property|{attr})="{re.escape(name)}"\s+content="([^"]*)"'
        m = re.search(pattern, content)
        if not m:
            # Try reversed attribute order
            pattern2 = rf'<meta\s+content="([^"]*)"\s+(?:property|{attr})="{re.escape(name)}"'
            m = re.search(pattern2, content)
        return m.group(1) if m else ''

    def get_title():
        m = re.search(r'<title>([^<]+)</title>', content)
        return html.unescape(m.group(1)) if m else ''

    # Determine if this is an article or general page
    og_type = get_meta('og:type', 'property') or 'website'

    # Get published date
    pub_date = get_meta('article:published_time', 'property') or ''

    # Get lastmod from file modification time if no pub date
    if not pub_date:
        mtime = os.path.getmtime(filepath)
        pub_date = datetime.fromtimestamp(mtime, tz=timezone.utc).strftime('%Y-%m-%d')

    # Check for author
    author = get_meta('author') or get_meta('article:author', 'property') or ''

    # Check for AI-generated label
    is_ai = 'AI-Generated' in content and 'label-ai' in content

    # Get categories
    categories = []
    if is_ai:
        categories.append('AI-Generated')
    elif author:
        categories.append('Human-Authored')

    # Detect topic categories from content
    if 'manifesto' in filepath.lower():
        categories.append('Manifesto')
    if 'philosophy' in get_meta('description').lower() or 'philosophical' in get_meta('description').lower():
        categories.append('Philosophy')
    if 'theology' in get_meta('description').lower() or 'christian framework' in get_meta('description').lower():
        categories.append('Theology')

    return {
        'title': get_title(),
        'description': get_meta('description'),
        'og_type': og_type,
        'pub_date': pub_date,
        'author': author,
        'is_article': og_type == 'article',
        'categories': categories,
    }


def get_priority(filename, og_type):
    """Assign sitemap priority based on page type."""
    if filename == 'index.html':
        return '1.0'
    if 'manifesto' in filename:
        return '0.9'
    if filename == 'about.html':
        return '0.7'
    if og_type == 'article':
        return '0.6'
    return '0.5'


def get_changefreq(filename):
    """Assign change frequency."""
    if filename == 'index.html':
        return 'weekly'
    return 'monthly'


def format_rfc822(date_str):
    """Convert YYYY-MM-DD to RFC 822 date for RSS."""
    dt = datetime.strptime(date_str, '%Y-%m-%d').replace(tzinfo=timezone.utc)
    return dt.strftime('%a, %d %b %Y %H:%M:%S +0000')


def generate_sitemap(pages):
    """Generate sitemap.xml content."""
    lines = [
        '<?xml version="1.0" encoding="UTF-8"?>',
        '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">',
    ]
    for filename, meta in pages:
        url = f'{DOMAIN}/' if filename == 'index.html' else f'{DOMAIN}/{filename}'
        lines.append('  <url>')
        lines.append(f'    <loc>{url}</loc>')
        lines.append(f'    <lastmod>{meta["pub_date"]}</lastmod>')
        lines.append(f'    <changefreq>{get_changefreq(filename)}</changefreq>')
        lines.append(f'    <priority>{get_priority(filename, meta["og_type"])}</priority>')
        lines.append('  </url>')

    # Add feed.xml entry
    today = datetime.now(timezone.utc).strftime('%Y-%m-%d')
    lines.append('  <url>')
    lines.append(f'    <loc>{DOMAIN}/feed.xml</loc>')
    lines.append(f'    <lastmod>{today}</lastmod>')
    lines.append('    <changefreq>weekly</changefreq>')
    lines.append('    <priority>0.3</priority>')
    lines.append('  </url>')

    lines.append('</urlset>')
    return '\n'.join(lines) + '\n'


def generate_feed(articles):
    """Generate feed.xml (RSS 2.0) content."""
    today_rfc822 = datetime.now(timezone.utc).strftime('%a, %d %b %Y %H:%M:%S +0000')

    lines = [
        '<?xml version="1.0" encoding="UTF-8"?>',
        '<rss version="2.0"',
        '     xmlns:atom="http://www.w3.org/2005/Atom"',
        '     xmlns:dc="http://purl.org/dc/elements/1.1/">',
        '  <channel>',
        f'    <title>{SITE_TITLE}</title>',
        f'    <link>{DOMAIN}/</link>',
        f'    <description>{html.escape(SITE_DESCRIPTION)}</description>',
        '    <language>en-us</language>',
        f'    <managingEditor>paul@truthandai.org ({AUTHOR})</managingEditor>',
        f'    <webMaster>paul@truthandai.org ({AUTHOR})</webMaster>',
        f'    <lastBuildDate>{today_rfc822}</lastBuildDate>',
        f'    <atom:link href="{DOMAIN}/feed.xml" rel="self" type="application/rss+xml"/>',
        '    <image>',
        f'      <url>{DOMAIN}/images/og-default.png</url>',
        f'      <title>{SITE_TITLE}</title>',
        f'      <link>{DOMAIN}/</link>',
        '    </image>',
        '',
    ]

    for filename, meta in articles:
        url = f'{DOMAIN}/{filename}'
        title_escaped = html.escape(html.unescape(meta['title'].split(' — ')[0].split(' &mdash; ')[0].strip()))
        desc_escaped = html.escape(meta['description'])
        pub_rfc822 = format_rfc822(meta['pub_date'])

        lines.append('    <item>')
        lines.append(f'      <title>{title_escaped}</title>')
        lines.append(f'      <link>{url}</link>')
        lines.append(f'      <guid isPermaLink="true">{url}</guid>')
        lines.append(f'      <pubDate>{pub_rfc822}</pubDate>')
        if meta['author']:
            lines.append(f'      <dc:creator>{html.escape(meta["author"])}</dc:creator>')
        lines.append(f'      <description>{desc_escaped}</description>')
        for cat in meta['categories']:
            lines.append(f'      <category>{html.escape(cat)}</category>')
        lines.append('    </item>')
        lines.append('')

    lines.append('  </channel>')
    lines.append('</rss>')
    return '\n'.join(lines) + '\n'


def main():
    # Collect all HTML files
    html_files = sorted([
        f for f in os.listdir(DOCS_DIR)
        if f.endswith('.html') and f not in EXCLUDE
    ])

    # Extract metadata
    pages = []
    for filename in html_files:
        filepath = os.path.join(DOCS_DIR, filename)
        meta = extract_meta(filepath)
        pages.append((filename, meta))
        print(f'  Found: {filename} ({meta["og_type"]})')

    # Sort: index first, then articles by date (newest first), then others
    def sort_key(item):
        filename, meta = item
        if filename == 'index.html':
            return (0, '')
        if 'manifesto' in filename:
            return (1, '')
        if filename == 'about.html':
            return (8, '')
        return (2, meta.get('pub_date', ''))

    pages.sort(key=sort_key)

    # Generate sitemap
    sitemap_content = generate_sitemap(pages)
    sitemap_path = os.path.join(DOCS_DIR, 'sitemap.xml')
    with open(sitemap_path, 'w', encoding='utf-8') as f:
        f.write(sitemap_content)
    print(f'\n  Generated sitemap.xml ({len(pages)} pages + feed.xml)')

    # Filter articles for RSS feed (only og:type=article pages)
    articles = [(f, m) for f, m in pages if m['is_article']]
    # Sort articles newest first
    articles.sort(key=lambda x: x[1]['pub_date'], reverse=True)

    feed_content = generate_feed(articles)
    feed_path = os.path.join(DOCS_DIR, 'feed.xml')
    with open(feed_path, 'w', encoding='utf-8') as f:
        f.write(feed_content)
    print(f'  Generated feed.xml ({len(articles)} articles)')

    print('\n  Done.')


if __name__ == '__main__':
    main()
